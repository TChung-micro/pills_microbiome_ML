# Load required packages
library(mlr3)
library(mlr3learners)
library(mlr3tuning)
library(paradox)
library(mlr3viz)
library(phyloseq)
library(ggplot2)
library(iml) # Changed from DALEX to iml for better compatibility

# Convert phyloseq object to data frame for ML
ps_df <- data.frame(
  t(otu_table(ps)),
  sal_detection = sample_data(ps)$sal_detection
)

# Ensure sal_detection is a factor
ps_df$sal_detection <- as.factor(ps_df$sal_detection)

# Create ML task - explicitly defining the target and features
task <- TaskClassif$new(
  id = "microbiome",
  backend = ps_df,
  target = "sal_detection",
  positive = levels(ps_df$sal_detection)[2]  # specify positive class
)

# Define learners with their respective parameter spaces
learners <- list(
  # Random Forest
  rf = lrn("classif.ranger",
    predict_type = "prob",
    importance = "permutation"
  )$train(task),  # Explicitly train on the task
  
  # XGBoost
  xgb = lrn("classif.xgboost",
    predict_type = "prob",
    importance = TRUE
  )$train(task),
  
  # Support Vector Machine
  svm = lrn("classif.svm",
    predict_type = "prob",
    type = "C-classification"
  )$train(task),
  
  # Neural Network
  nnet = lrn("classif.nnet",
    predict_type = "prob"
  )$train(task),
  
  # Elastic Net
  glmnet = lrn("classif.glmnet",
    predict_type = "prob"
  )$train(task),
  
  # k-Nearest Neighbors
  knn = lrn("classif.kknn",
    predict_type = "prob"
  )$train(task),
  
  # Decision Tree
  rpart = lrn("classif.rpart",
    predict_type = "prob"
  )$train(task),
  
  # Naive Bayes
  nb = lrn("classif.naive_bayes",
    predict_type = "prob"
  )$train(task),
  
  # Linear Discriminant Analysis
  lda = lrn("classif.lda",
    predict_type = "prob"
  )$train(task),
  
  # Quadratic Discriminant Analysis
  qda = lrn("classif.qda",
    predict_type = "prob"
  )$train(task)
)

# Define parameter spaces for tuning
search_spaces <- list(
  # Random Forest parameters
  rf = ps(
    num.trees = p_int(lower = 100, upper = 1000),
    mtry = p_int(lower = 2, upper = 20),
    min.node.size = p_int(lower = 1, upper = 10)
  ),
  
  # XGBoost parameters
  xgb = ps(
    nrounds = p_int(lower = 100, upper = 1000),
    eta = p_dbl(lower = 0.01, upper = 0.3),
    max_depth = p_int(lower = 3, upper = 10),
    subsample = p_dbl(lower = 0.5, upper = 1),
    colsample_bytree = p_dbl(lower = 0.5, upper = 1)
  ),
  
  # Other parameters remain the same...
  # [previous parameter definitions for other algorithms]
)

# Set up cross-validation
cv5 = rsmp("cv", folds = 5)

# Initialize results and importance storage
results <- list()
importance_list <- list()

# Perform tuning and calculate variable importance for each algorithm
for (name in names(learners)) {
  # Create AutoTuner with explicit task
  at <- AutoTuner$new(
    learner = learners[[name]],
    resampling = cv5,
    measure = msr("classif.auc"),
    search_space = search_spaces[[name]],
    terminator = trm("evals", n_evals = 50),
    tuner = tnr("random_search")
  )
  
  # Train and evaluate with explicit task
  at$train(task)
  results[[name]] <- resample(task, at, cv5, store_models = TRUE)
  
  # Create predictor object using iml
  X <- as.data.frame(ps_df[, -ncol(ps_df)])  # Features
  predictor <- Predictor$new(
    model = at,
    data = X,
    y = ps_df$sal_detection,
    task = "classification"
  )
  
  # Calculate feature importance
  importance <- FeatureImp$new(
    predictor,
    loss = "ce"
  )
  
  importance_list[[name]] <- data.frame(
    feature = importance$results$feature,
    importance = importance$results$importance
  )
}

# Compare results
scores <- sapply(results, function(x) x$aggregate(msr("classif.auc")))
print(sort(scores, decreasing = TRUE))

# Plot model performance comparison
boxplot(
  lapply(results, function(x) x$score(msr("classif.auc"))),
  main = "Model Performance Comparison",
  ylab = "AUC",
  las = 2
)

# Plot variable importance for each algorithm
for (name in names(importance_list)) {
  imp_data <- importance_list[[name]]
  
  p <- ggplot(imp_data[order(-imp_data$importance)[1:20], ], 
              aes(x = reorder(feature, importance), y = importance)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    theme_minimal() +
    labs(
      title = paste("Top 20 Important Features -", name),
      x = "Features",
      y = "Importance Score"
    )
  
  print(p)
}

# Create a combined importance plot
combined_importance <- do.call(rbind, lapply(names(importance_list), function(name) {
  imp_data <- importance_list[[name]]
  imp_data$model <- name
  return(imp_data)
}))

avg_importance <- aggregate(importance ~ feature, data = combined_importance, FUN = mean)

ggplot(avg_importance[order(-avg_importance$importance)[1:20], ],
       aes(x = reorder(feature, importance), y = importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  labs(
    title = "Top 20 Important Features (Average across all models)",
    x = "Features",
    y = "Average Importance Score"
  )




